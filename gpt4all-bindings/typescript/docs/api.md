<!-- Generated by documentation.js. Update this documentation by updating the source code. -->

### Table of Contents

*   [download][1]
    *   [Parameters][2]
    *   [Examples][3]
*   [DownloadOptions][4]
    *   [location][5]
    *   [debug][6]
    *   [url][7]
*   [DownloadController][8]
    *   [cancel][9]
    *   [promise][10]
*   [ModelType][11]
*   [ModelFile][12]
    *   [gptj][13]
    *   [llama][14]
    *   [mpt][15]
*   [LLModel][16]
    *   [type][17]
    *   [name][18]
    *   [stateSize][19]
    *   [threadCount][20]
    *   [setThreadCount][21]
        *   [Parameters][22]
    *   [raw\_prompt][23]
        *   [Parameters][24]
    *   [isModelLoaded][25]
*   [createCompletion][26]
    *   [Parameters][27]
    *   [Examples][28]
*   [CompletionOptions][29]
    *   [verbose][30]
    *   [hasDefaultHeader][31]
    *   [hasDefaultFooter][32]
*   [PromptMessage][33]
    *   [role][34]
    *   [message][35]
*   [prompt\_tokens][36]
*   [completion\_tokens][37]
*   [total\_tokens][38]
*   [CompletionReturn][39]
    *   [model][40]
    *   [usage][41]
    *   [choices][42]
*   [role][43]
*   [content][44]
*   [CompletionChoice][45]
    *   [message][46]
*   [LLModelPromptContext][47]
    *   [logits\_size][48]
    *   [tokens\_size][49]
    *   [n\_past][50]
    *   [n\_ctx][51]
    *   [n\_predict][52]
    *   [top\_k][53]
    *   [top\_p][54]
    *   [temp][55]
    *   [n\_batch][56]
    *   [repeat\_penalty][57]
    *   [repeat\_last\_n][58]
    *   [context\_erase][59]
*   [createTokenStream][60]
    *   [Parameters][61]

## download

Initiates the download of a model file of a specific model type.
By default this downloads without waiting. use the controller returned to alter this behavior.

### Parameters

*   `model` **[ModelFile][12]** The model file to be downloaded.
*   `options` **[DownloadOptions][4]** to pass into the downloader. Default is { location: (cwd), debug: false }.

### Examples

```javascript
const controller = download('ggml-gpt4all-j-v1.3-groovy.bin')
controller.promise().then(() => console.log('Downloaded!'))
```

Returns **[DownloadController][8]** object that allows controlling the download process.

## DownloadOptions

Options for the model download process.

### location

location to download the model.
Default is process.cwd(), or the current working directory

Type: [string][62]

### debug

Debug mode -- check how long it took to download in seconds

Type: [boolean][63]

### url

Default url = [https://gpt4all.io/models][64]
This property overrides the default.

Type: [string][62]

## DownloadController

Model download controller.

### cancel

Cancel the request to download from gpt4all website if this is called.

Type: function (): void

### promise

Convert the downloader into a promise, allowing people to await and manage its lifetime

Type: function (): [Promise][65]\<void>

## ModelType

Type of the model

Type: (`"gptj"` | `"llama"` | `"mpt"`)

## ModelFile

Full list of models available

### gptj

List of GPT-J Models

Type: (`"ggml-gpt4all-j-v1.3-groovy.bin"` | `"ggml-gpt4all-j-v1.2-jazzy.bin"` | `"ggml-gpt4all-j-v1.1-breezy.bin"` | `"ggml-gpt4all-j.bin"`)

### llama

List Llama Models

Type: (`"ggml-gpt4all-l13b-snoozy.bin"` | `"ggml-vicuna-7b-1.1-q4_2.bin"` | `"ggml-vicuna-13b-1.1-q4_2.bin"` | `"ggml-wizardLM-7B.q4_2.bin"` | `"ggml-stable-vicuna-13B.q4_2.bin"` | `"ggml-nous-gpt4-vicuna-13b.bin"`)

### mpt

List of MPT Models

Type: (`"ggml-mpt-7b-base.bin"` | `"ggml-mpt-7b-chat.bin"` | `"ggml-mpt-7b-instruct.bin"`)

## LLModel

LLModel class representing a language model.
This is a base class that provides common functionality for different types of language models.

### type

either 'gpt', mpt', or 'llama'

Returns **[ModelType][11]**&#x20;

### name

The name of the model.

Returns **[ModelFile][12]**&#x20;

### stateSize

Get the size of the internal state of the model.
NOTE: This state data is specific to the type of model you have created.

Returns **[number][66]** the size in bytes of the internal state of the model

### threadCount

Get the number of threads used for model inference.
The default is the number of physical cores your computer has.

Returns **[number][66]** The number of threads used for model inference.

### setThreadCount

Set the number of threads used for model inference.

#### Parameters

*   `newNumber` **[number][66]** The new number of threads.

Returns **void**&#x20;

### raw\_prompt

Prompt the model with a given input and optional parameters.
This is the raw output from std out.
Use the prompt function exported for a value

#### Parameters

*   `q` **[string][62]** The prompt input.
*   `params` **Partial<[LLModelPromptContext][47]>?** Optional parameters for the prompt context.

Returns **any** The result of the model prompt.

### isModelLoaded

Whether the model is loaded or not.

Returns **[boolean][63]**&#x20;

## createCompletion

The nodejs equivalent to python binding's chat\_completion

### Parameters

*   `llmodel` **[LLModel][16]** The language model object.
*   `messages` **[Array][67]<[PromptMessage][33]>** The array of messages for the conversation.
*   `options` **[CompletionOptions][29]** The options for creating the completion.

### Examples

```javascript
const llmodel = new LLModel(model)
const messages = [ 
{ role: 'system', message: 'You are a weather forecaster.' },
{ role: 'user', message: 'should i go out today?' } ]
const completion = await createCompletion(llmodel, messages, {
 verbose: true,
 temp: 0.9,
})
console.log(completion.choices[0].message.content)
// No, it's going to be cold and rainy.
```

Returns **[CompletionReturn][39]** The completion result.

## CompletionOptions

**Extends Partial\<LLModelPromptContext>**

The options for creating the completion.

### verbose

Indicates if verbose logging is enabled.

Type: [boolean][63]

### hasDefaultHeader

Indicates if the default header is included in the prompt.

Type: [boolean][63]

### hasDefaultFooter

Indicates if the default footer is included in the prompt.

Type: [boolean][63]

## PromptMessage

A message in the conversation, identical to OpenAI's chat message.

### role

The role of the message.

Type: (`"system"` | `"assistant"` | `"user"`)

### message

The message content.

Type: [string][62]

## prompt\_tokens

The number of tokens used in the prompt.

Type: [number][66]

## completion\_tokens

The number of tokens used in the completion.

Type: [number][66]

## total\_tokens

The total number of tokens used.

Type: [number][66]

## CompletionReturn

The result of the completion, similar to OpenAI's format.

### model

The model name.

Type: [ModelFile][12]

### usage

Token usage report.

Type: {prompt\_tokens: [number][66], completion\_tokens: [number][66], total\_tokens: [number][66]}

### choices

The generated completions.

Type: [Array][67]<[CompletionChoice][45]>

## role

The role of the message.

Type: `"assistant"`

## content

The message content.

Type: [string][62]

## CompletionChoice

A completion choice, similar to OpenAI's format.

### message

Response message

Type: {role: `"assistant"`, content: [string][62]}

## LLModelPromptContext

Model inference arguments for generating completions.

### logits\_size

The size of the raw logits vector.

Type: [number][66]

### tokens\_size

The size of the raw tokens vector.

Type: [number][66]

### n\_past

The number of tokens in the past conversation.

Type: [number][66]

### n\_ctx

The number of tokens possible in the context window.

Type: [number][66]

### n\_predict

The number of tokens to predict.

Type: [number][66]

### top\_k

The top-k logits to sample from.

Type: [number][66]

### top\_p

The nucleus sampling probability threshold.

Type: [number][66]

### temp

The temperature to adjust the model's output distribution.

Type: [number][66]

### n\_batch

The number of predictions to generate in parallel.

Type: [number][66]

### repeat\_penalty

The penalty factor for repeated tokens.

Type: [number][66]

### repeat\_last\_n

The number of last tokens to penalize.

Type: [number][66]

### context\_erase

The percentage of context to erase if the context window is exceeded.

Type: [number][66]

## createTokenStream

TODO: Help wanted to implement this

### Parameters

*   `llmodel` **[LLModel][16]**&#x20;
*   `messages` **[Array][67]<[PromptMessage][33]>**&#x20;
*   `options` **[CompletionOptions][29]**&#x20;

Returns **function (ll: [LLModel][16]): AsyncGenerator<[string][62]>**&#x20;

[1]: #download

[2]: #parameters

[3]: #examples

[4]: #downloadoptions

[5]: #location

[6]: #debug

[7]: #url

[8]: #downloadcontroller

[9]: #cancel

[10]: #promise

[11]: #modeltype

[12]: #modelfile

[13]: #gptj

[14]: #llama

[15]: #mpt

[16]: #llmodel

[17]: #type

[18]: #name

[19]: #statesize

[20]: #threadcount

[21]: #setthreadcount

[22]: #parameters-1

[23]: #raw_prompt

[24]: #parameters-2

[25]: #ismodelloaded

[26]: #createcompletion

[27]: #parameters-3

[28]: #examples-1

[29]: #completionoptions

[30]: #verbose

[31]: #hasdefaultheader

[32]: #hasdefaultfooter

[33]: #promptmessage

[34]: #role

[35]: #message

[36]: #prompt_tokens

[37]: #completion_tokens

[38]: #total_tokens

[39]: #completionreturn

[40]: #model

[41]: #usage

[42]: #choices

[43]: #role-1

[44]: #content

[45]: #completionchoice

[46]: #message-1

[47]: #llmodelpromptcontext

[48]: #logits_size

[49]: #tokens_size

[50]: #n_past

[51]: #n_ctx

[52]: #n_predict

[53]: #top_k

[54]: #top_p

[55]: #temp

[56]: #n_batch

[57]: #repeat_penalty

[58]: #repeat_last_n

[59]: #context_erase

[60]: #createtokenstream

[61]: #parameters-4

[62]: https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String

[63]: https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Boolean

[64]: https://gpt4all.io/models

[65]: https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Promise

[66]: https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number

[67]: https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Array
